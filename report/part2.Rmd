# Part 2: Simple linear regression with the `mylm` package
```{r, eval = TRUE, echo=FALSE}
library(car)
data(SLID, package = "carData")
SLID <- SLID[complete.cases(SLID),]
```

### a) Estimation of Coefficients
TODO: Write down all formulas needed to implement print and summary

The parameter estimates are calculated by the general formula 
$$
\hat{\mathbf{\beta}} = (X^TX)^{-1}X^T\mathbf{y}.
$$
```{r eval = TRUE}
library(mylm)
model1 <- mylm(wages ~ education, data = SLID)
print(model1)
```

```{r eval = TRUE}
model1b <- lm(wages ~ education, data = SLID)
print(model1b)
```
We see that we get the same output, so our function is good so far.

### b) Estimation of Covariance Matrix
To calculate the covariance matrix, we use that
$$
\widehat{\text{Cov}}(\mathbf{\hat\beta}) = \hat\sigma^2({\bf X}^T{\bf X})^{-1}.
$$
This gives us the matrix
```{r}
model1$covmatrix
```
By implementing the `summary` function, we are able to calculate the estimates and standard errors of the intercept and regression coefficient for this model, and test their significance. 
The interpretation of the parameter estimates is that as the corresponding $x_i$ increases by one, $y$ is expected to increase by $\hat{\beta}_i$.
This gives us the estimate for each response as $X^T\hat\beta$ and the standard errors as 
$$
\hat{\mathbf{e}} = \mathbf{Y}-X^T\hat{\beta}.
$$

If model assumptions hold, the coefficient estimators are distributed as follows:

$$
\hat{\beta}\sim N_{p}(\beta,\sigma^2({\bf X}^T{\bf X})^{-1})
$$

To obtain the test statistic for each of the parameters, we calculate the test statistic under the null hypothesis that the true parameter value is 0. That is 
$$
z_i = \frac{\hat{\beta}_i-0}{\sqrt{ \hat{\sigma^2} ({{\bf X}^T{\bf X})^{-1}}_{ii}}},
$$
where $\hat{\sigma^2}$ is given by
$$
\hat{\sigma}^2=\frac{1}{n-p}({\bf Y}-{\bf X}\hat{\beta})^T({\bf Y}-{\bf X}\hat{\beta})=\frac{\text{SSE}}{n-p},
$$
with
$$
\frac{(n-p)\hat{\sigma}^2}{\sigma^2} \sim \chi^2_{n-p}.
$$
We approximate the distribution of $Z_i$ as a normal distribution, although, given the distribution of $\hat{\sigma}^2$, it is actually distributed according to a $t$-distribution $n-p$ degrees of freedom. However, when $n$ is large it is reasonable to approximate this distribution by a normal distribution. The corresponding p-value for the test statistic is the probability of observing the test statistic or a more extreme outcome, i.e.
$$
p_i = P( Z >|z_i|).
$$

```{r}
summary(model1)
```
We see that both `z-value`s are very large for a standard normal distribution, and correspondingly, the p-values are very small, which seems quite reasonable.
We may also compare to the model implemented with the regular `lm` function:
```{r}
summary(model1b)
```
We see that the results are the same, except for the z-values, which are not supposed to be the same.

### c) Residual Plot
We then implement the plot function, in order to plot the fitted values against the residuals.
```{r eval = TRUE}
plot(model1)
```

It seems that the mean of the residuals is approximately 0 - that is good, as this is one of our assumptions.
However, there is asymmetry, as there is a thicker positive tail. We notice a hint of multiplicative residuals, which means the residuals are not independent of the fitted values.

### d) Residual Analysis

**Q:** What is the residual sum of squares (SSE) and the degrees of freedom for this model? 

**A:** SSE is `r toString(round(model1$SSE, 0))`, with number of degrees of freedom $n-p =$ `r length(model1$fitted_values)` $-$ `r length(model1$coefficients)` = `r length(model1$fitted_values) - length(model1$coefficients)`


**Q:** What is total sum of squares (SST) for this model? Test the significance of the regression using a $\chi^2$-test. 

**A:** SST is `r toString(round(model1$SST, 0))`, with p-value of the $\chi^2$-test equal to `r model1$p_chi`

In order to test the significance of regression we calculate the F-statistic

$$
F = \frac{n-p}{k}\frac{R^2}{1-R^2},
$$
where $R^2 = 1-\frac{SSE}{SST}$ is the coefficient of determination (see next point). Since this has a Fisher distribution with $k$ and $n-p$ degrees of freedom, we know that $k\cdot F$ has a chi squared distribution with $k$ degrees of freedom. This is what we use to calculate the p-value.


**Q:** What is the relationship between the \(\chi^2\)- and \(z\)-statistic in simple linear regression? Find the critical value(s) for both tests.

**A:** In the simple linear regression case we have that the \(\chi^2\)-statistic is the squared of the \(z\)-statistic. This we can also see from the `summary` of `model1`: 
```{r}
model1$chistatistic
model1$zvalues[2]^2
```


### e) Coefficient of Determination
The coefficient of determination $R^2$ gives the proportion of variance in the data explained by the model. We calculate it as mentioned above,

$$
R^2 = 1-\frac{SSE}{SST},
$$

```{r eval = TRUE}
model1$Rsq
```
