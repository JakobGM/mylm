--- 
title: "TMA4315: Compulsory exercise 3" 
subtitle: "Group 13: Magnus Liland, Jakob Gerhard Martinussen and Emma Skarstein"
date: "`r format(Sys.time(), '%d.%m.%Y')`" # the current date, can be regular text as well
output: 
  # html_document:
  #   toc: yes
  #   toc_depth: 3
  #   toc_float: yes
  pdf_document:
   toc: false
   toc_depth: 2
---

```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# The dataset

We will use a *simulated* dataset with clustered data. This data is generated from a fitted model to the `jsp` dataset in the `faraway` R-package.

The following variables are made available:

* `school`: 50 schools, with code 1-50.
* `gender`: A factor with levels boy, girl.
* `social`: Social class of the father, categorical.
    Original class 1-2 = S1, 3-4 = S2, 5-6 = S3 and 7-9 = S4
    Note that these are not ordered and S1 is not necessarily higher or lower class than S2!
* `raven`: Test score (centered around 0).
* `math`: Math score (centered around 0).

We will use `math` as response, and group the data by school.

```{r, echo = TRUE}
dataset <- read.table("https://www.math.ntnu.no/emner/TMA4315/2018h/jsp2.txt", header = TRUE)
```

The number of schools is 49, as we omit school number 43 due to the lack of measurements for our particular subset.

# Analysis

## a) Fitting a linear model

### Visualizing the dataset

First we want to explore the dataset. We will group the data based on `gender` and only include covariates `social`, `raven`, and `math`.

```{r}
library(GGally)
ggpairs(
  data = dataset,
  mapping = aes(col = gender, alpha = 0.7),
  columns = c("social", "raven", "math"),
  legend = 1,
)
```

Some observations can be made:

* A positive correlation can be observed between `raven` and `math` score for both genders, although the correlation is marginally stronger for boys. The Raven test [measures abstract reasoning capabilities](https://en.wikipedia.org/wiki/Raven%27s_Progressive_Matrices), so this makes intuitive sense.
* Girls perform, on average, better than boys on the math test. The weakest math students are mainly boys, and the strongest math students are mainly girls, as well.

### Fitting a linear model


We will now fit a _linear model_ with `math` as response, and `raven` and `gender` as covariates. The model for the $k$th student is therefore

$$
Y_k = {\bf x}_k^\text{T} \pmb{\beta} + \varepsilon_k
$$

where we assume that the $\varepsilon_k$s are independent (between students), and have mean 0 and variance $\sigma^2$ for all students. Some explanations for the terms in the model:

* $Y_k$ is the **predicted** `math` score for student $k$. It is assumed to be normally distributed with mean ${\bf x}_k \pmb \beta$ and variance $\sigma^2$.
* ${\bf x}_k$ is a $3 \times 1$ **covariate vector** for student $k$. For instance, if student $k$ is a boy with `raven` score 5, then ${\bf x}_k = [1, 0, 5]^\text{T}$. Observe that ${\bf x}_{k0}$ is always 0, since this covariate "represents" the intercept of the model.
* $\pmb{\beta}$ is the **regression coefficients** of the fitted model. $\pmb{\beta} = [\beta_{\text{intercept}}, \beta_{\text{raven}}, \beta_{\text{gender}}]^\text{T}$. For example, you would expect a given student to improve their `math` score by $\beta_{\text{raven}}$ if the student improves their `raven` score by one, everything else being equal.
* $\varepsilon_k$ is the **error** in the modelled relationship for student $k$. The important part here is that $\varepsilon_k$ is assumed to be independently and identically distributed with mean $0$ and variance $\sigma^2$ for *all* students.

Now let's estimate the regression coefficients of this linear model and inspect its summary:

```{r}
model <- lm(math ~ raven + gender, data=dataset)
summary(model)
```

The parameter estimates seem to be significant. We have $\beta \approx [-1.3131, 0.1965, 2.5831]^{\text{T}}$. A girl is expected to achieve ~2.5 additional `math` points compared to a boy with the same number of `raven` points. Additionally, a student is expected to score ~0.2 additional `math` points for every `raven` point he or she achieves.

We can rewrite this model as two separate models, one for boys and one for girls, making these relationships clear. For boys

$$
Y_k = 0.1965 \cdot x_{k, \text{raven}} - 1.3131,
$$

and likewise for girls

$$
Y_k = 0.1965 \cdot x_{k, \text{raven}} + 1.225.
$$

Making the statistical advantage of female students apparent.

With this model we investigate how two different factors affect the mathematical capabilities of a student.
The first of these factors is gender. Gender does indeed seem to have a significant effect, as girls seem to achieve better math scores than boys.
Secondly, there is a positive correlation between the performing well on the "Raven" test and the mathematics test. As the Raven test measures abstract, cognitive capabilities, this comes at no large surprise.


---

## b) Fitting a random intercept model

### Explanation of the model

We will now fit a *random intercept model* with `school` as the random intercept. For school $i$ we study the measurement model:

$$
{\bf Y}_{i} = {\bf X}_i\beta + {\bf 1} \gamma_{0i} + \varepsilon_{i}
$$

We denote the number of students in school $i$ as $n_i$, often called a **cluster**.
Here follows some explanations of the different parts of the model:

* ${\bf Y}_{i}$ is the $n_1 \times 1$ **response vector** for school $i$, containing the math scores of each student enrolled in that school.
* ${\bf X}_i$ is the $n_i \times 3$ **design matrix** of the model, containing "population covariates" for each school $i$s student on each row.
* The $3 \times 1$ vector $\beta$ contains the **fixed effects** of the model. These "population effects" are common amongst *all* the schools.
* The $n_1 \times 1$ vector ${\bf 1} \gamma_{0i}$ contains the **random effects** of the model. Specifically in this case, since we have fitted a *random intercept model*, we have a **random intercept** and *no* random slope. ${\bf 1}$ is a $n_i \times 1$ vector containing solely ones, while $\gamma_{0i} \in â„$. With other words, ${\bf 1} \gamma_{0i} = [\gamma_{0i}, ..., \gamma_{0i}]^\text{T}$.
* The $n_i \times 1$ vector $\varepsilon_{i}$ contains the **random errors** in the model prediction.


It is important to note that we have made the following distributional assumptions:

* $\gamma_{0i} \sim \text{N}_{1}(0, \tau_0^2)$, where $\tau_0^2$ is a scalar which must be estimated.
* $\varepsilon_{i} \sim \text{N}_{n_i}(0, \sigma^2 I)$, where $\sigma^2$, the variance, also must be estimated.


We have also assumed that there is zero correlation between responses in *different* clusters, and we only have **intraclass correlation**. With other words, $\mathrm{Cov}(Y_{ij}, Y_{kl}) = 0$ for $i \neq k$.
